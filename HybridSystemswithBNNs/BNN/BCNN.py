# -*- coding: utf-8 -*-
"""CBNNlarq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nL9RXX1Z6K5chuIm42cKDBfV1B7eH-Qr
"""

pip install larq

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/BBMge60step01of1000.mat')

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/BBMge60step01of1000.mat')
    


t = data['T'].flatten()[:,None]
#t=t[:403]
t=t[:1000]
x = data['H1'].flatten()[:,None]
x=x.reshape(1000, 200001)

#xx=np.zeros((16,2782))
xx=np.zeros((1000,1000))
h=np.zeros((1000,1))
i=0
for x1 in x:
  xx[i]=x1[:1000]
  h[i]=x1[0]
  i=i+1
  
 
 
H, T = np.meshgrid(h,t)

H=(H-60)/4.995 - 1 #(H-min)/((max-min)/2)-1
 
T=-1+T/2.4975 #T/(max/2)-1
 
print("H", H.shape)
print(H)

print("T", T.shape)
print(T)

x_train_val= np.hstack((H.flatten()[:,None], T.flatten()[:,None]))
print("x_train_val", x_train_val.shape)
print(x_train_val)
#x_train_val= np.hstack((H, T))

#y_train_val=xx.T.flatten()[:,None]
y_train_val=xx.T.flatten()[:,None]
#y_train_val=-1+(y_train_val-20)/37.5


x_train=x_train_val[:-100000,:]
print("x_train", x_train.shape)
x_train=x_train.reshape((9000, 10, 10, 2))
print("x_train", x_train.shape)
print(x_train)
x_val=x_train_val[-100000:,:]
print("x_val", x_val.shape)
x_val=x_val.reshape((1000, 10, 10, 2))


 
 
y_train=y_train_val[:-100000,:]
y_train=y_train.reshape((9000, 100))
y_val=y_train_val[-100000:,:]
y_val=y_val.reshape((1000, 100))

print("y_val", y_val.shape)

import larq as lq

# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer="ste_sign",
              kernel_quantizer="ste_sign",
              kernel_constraint="weight_clip")

model = tf.keras.models.Sequential()

# In the first layer we only quantize the weights and not the input
# One has bigger error with output size 32
model.add(lq.layers.QuantConv2D(64, (2, 2),
                                kernel_quantizer="ste_sign",
                                kernel_constraint="weight_clip",
                                use_bias=False,
                                input_shape=(10, 10, 2)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantConv2D(64, (2, 2), use_bias=False, **kwargs))
#model.add(lq.layers.QuantConv2D(64, (2, 2), **kwargs))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))


model.add(lq.layers.QuantConv2D(64, (2, 2), use_bias=False, padding='same', **kwargs)) #[3,3,64,64]
#model.add(lq.layers.QuantConv2D(128, (2, 2), padding='same', input_shape=(1,1,6), **kwargs)) #[3,3,64,64]
model.add(tf.keras.layers.BatchNormalization(scale=False))
model.add(tf.keras.layers.Flatten())

model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs)) #use_bias no impact
#model.add(lq.layers.QuantDense(128, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
#model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Activation("softmax"))


model.compile(optimizer="adam", loss="mean_squared_error", metrics=["mean_squared_error"])

for i in range(600):

  model.fit(x_train, y_train, epochs=1, batch_size=128, validation_data=(x_val, y_val))

#model.fit(x_train, y_train, batch_size=32, epochs=18)

  #print(x_val)

  y_pred=model.predict(x_val)
  y_pred1=y_pred.flatten()[:,None]
  print("y_pred1.shape", y_pred1.shape)
  print("y_pred.shape", y_pred.shape)

  y_val1=y_val.flatten()[:,None]

  print("y_pred", y_pred)
 
  print("y_val", y_val)

  error = np.linalg.norm(y_val1-y_pred1,2)/np.linalg.norm(y_val1,2)
  print('Error u: %e' % (error))

#to do: dataset size is too small, 2200, should added up to around 60000 (as for minist)

import larq as lq

# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer="ste_sign",
              kernel_quantizer="ste_sign",
              kernel_constraint="weight_clip",
              use_bias=False)

model = tf.keras.models.Sequential([
    # In the first layer we only quantize the weights and not the input
    lq.layers.QuantConv2D(128, 3,
                          kernel_quantizer="ste_sign",
                          kernel_constraint="weight_clip",
                          use_bias=False,
                          input_shape=(10, 10, 2)),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    lq.layers.QuantConv2D(128, 3, padding="same", **kwargs),
    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),
    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1)),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    lq.layers.QuantConv2D(256, 3, padding="same", **kwargs),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    lq.layers.QuantConv2D(256, 3, padding="same", **kwargs),
    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),
    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1)),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    lq.layers.QuantConv2D(512, 3, padding="same", **kwargs),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    lq.layers.QuantConv2D(512, 3, padding="same", **kwargs),
    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),
    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1)),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),
    tf.keras.layers.Flatten(),

    #lq.layers.QuantDense(1024, **kwargs),
    lq.layers.QuantDense(256, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    #lq.layers.QuantDense(1024, **kwargs),
    lq.layers.QuantDense(256, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),

    lq.layers.QuantDense(100, **kwargs),
    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),
    #tf.keras.layers.Activation("softmax")
])

model.compile(optimizer="adam", loss="mean_squared_error", metrics=["mean_squared_error"])

for i in range(6000):

  model.fit(x_train, y_train, epochs=1, batch_size=200, validation_data=(x_val, y_val))

  y_pred=model.predict(x_val)
  y_pred1=y_pred.flatten()[:,None]
  print("y_pred1.shape", y_pred1.shape)
  print("y_pred.shape", y_pred.shape)

  y_val1=y_val.flatten()[:,None]

  print("y_pred", y_pred)
 
  print("y_val", y_val)

  error = np.linalg.norm(y_val1-y_pred1,2)/np.linalg.norm(y_val1,2)
  print('Error u: %e' % (error))

#to do: dataset size is too small, 2200, should added up to around 60000 (as for minist)