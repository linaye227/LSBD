# -*- coding: utf-8 -*-
"""BNNLARQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16tTPwfTZml4MzCxYGL3kC2zFXAzsMZv4
"""

pip install larq

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/NLS.mat')
    
t = data['tt'].flatten()[:,None]
x = data['x'].flatten()[:,None]
Exact = data['uu']
Exact_u = np.real(Exact)
#Exact_u=Exact_u*10
Exact_v = np.imag(Exact)
#Exact_v=Exact_v*10
Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)

X, T = np.meshgrid(x,t)

#X = X / 5
#T = T / (np.pi/4) -1

#print("X", X)
#print("T", T)

#X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))
X_star = np.hstack((X.T.flatten()[:,None], T.T.flatten()[:,None]))
 
#u_star = Exact_u.T.flatten()[:,None]
#v_star = Exact_v.T.flatten()[:,None]
u_star = Exact_u.flatten()[:,None]
v_star = Exact_v.flatten()[:,None]
Y_star = np.hstack((u_star, v_star))
print(Y_star)
 
h_star = Exact_h.T.flatten()[:,None]

x_train=X_star[:-10050,:]
x_val=X_star[-10050:,:]
 
#x_train= x_train.reshape((41456, 2, 1))
#x_val= x_val.reshape((10000, 2, 1))

y_train=Y_star[:-10050,:]
#print("x_train", x_train)
#print("y_train", y_train)
y_val=Y_star[-10050:,:]
print("x_val", x_val.shape)
print(x_val)
print("y_val", y_val.shape)
print(y_val)

#x_train, x_val= x_train / 5, x_val / 5
#print("x_train", x_train)

#y_train= y_train.reshape((41456, 2, 1))
##y_val= y_val.reshape((10000, 2, 1))


###########################
'''
idx_x = np.random.choice(x.shape[0], 50, replace=False)
x0 = x[idx_x,:]
u0 = Exact_u[idx_x,0:1]
v0 = Exact_v[idx_x,0:1]

idx_t = np.random.choice(t.shape[0], 50, replace=False)
tb = t[idx_t,:]
print("tb", tb)'''


'''
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Preprocess the data (these are NumPy arrays)
x_train = x_train.reshape(60000, 784).astype("float32") / 255
x_test = x_test.reshape(10000, 784).astype("float32") / 255

y_train = y_train.astype("float32")
y_test = y_test.astype("float32")

# Reserve 10,000 samples for validation
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]
'''

import larq as lq
# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer="ste_sign",
              kernel_quantizer="ste_sign",
              kernel_constraint="weight_clip")

model = tf.keras.models.Sequential()

# In the first layer we only quantize the weights and not the input
model.add(lq.layers.QuantDense(100,
                                kernel_quantizer="ste_sign",
                                kernel_constraint="weight_clip",
                                use_bias=False,
                                input_shape=(2,)))
#model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
#model.add(tf.keras.layers.MaxPooling2D(2))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Flatten())

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
model.add(lq.layers.QuantDense(2, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Activation("softmax"))


#model.compile(optimizer='adam',
#             loss='mean_squared_logarithmic_error',
#              metrics=['accuracy',])

#model.fit(x_train, y_train, batch_size=32, epochs=18)

model.compile(optimizer="Nadam", loss="mean_squared_error", metrics=["mean_squared_error"])
for i in range(600):

  model.fit(x_train, y_train, epochs=1, batch_size=32, validation_data=(x_val, y_val))

#model.fit(x_train, y_train, batch_size=32, epochs=18)

  #print(x_val)

  y_pred=model.predict(x_val)
  print("y_val", y_val)
  print("y_pred", y_pred)
 
 

  error = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
  print('Relative L2 Norm: %e' % (error))





'''result = model.fit(x_train, y_train, epochs=9, batch_size=32, validation_data=(x_val, y_val))

# enable early stopping based on mean_squared_error
#earlystopping=EarlyStopping(monitor="mean_squared_error", patience=40, verbose=1, mode='auto')
# fit model
#result = model.fit(x_train, y_train, epochs=240, batch_size=5, validation_data=(x_test, y_test), callbacks=[earlystopping])

y_pred=model.predict(x_val)

print("y_pred", y_pred.shape)
print(y_pred)
print("y_val", y_val)


with lq.context.quantized_scope(True):
    model.save("binary_model.h5")  # save binary weights
    weights = model.get_weights()
    print(weights)

#test_loss, test_acc = model.evaluate(x_val, y_val)

#print(f"Test accuracy {test_acc * 100:.2f} %")

error_y = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
 
print('Error y: %e' % (error_y))'''

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/NLS.mat')
    
t = data['tt'].flatten()[:,None]
x = data['x'].flatten()[:,None]
print("t.shape", t.shape)
print("x.shape", x.shape)
print(x)
Exact = data['uu']
Exact_u = np.real(Exact)
Exact_v = np.imag(Exact)
Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)

X, T = np.meshgrid(x,t)

X_star = np.hstack((X.T.flatten()[:,None], T.T.flatten()[:,None]))
 
u_star = Exact_u.flatten()[:,None]
v_star = Exact_v.flatten()[:,None]
Y_star = np.hstack((u_star, v_star))
 
h_star = Exact_h.T.flatten()[:,None]

x_train=X_star[:-10050,:]
x_val=X_star[-10050:,:]
 
 
y_train=Y_star[:-10050,:]
y_val=Y_star[-10050:,:]
print("x_val", x_val)

print("y_val", y_val)

import larq as lq
# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer=None,
              kernel_quantizer=None,
              kernel_constraint=None)

model = tf.keras.models.Sequential()

# In the first layer we only quantize the weights and not the input
model.add(lq.layers.QuantDense(100,   
                               input_quantizer=None, 
                               kernel_quantizer=None,                 
                               use_bias=False,
                                input_shape=(2,)))
#model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
#model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Flatten())

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
model.add(lq.layers.QuantDense(2, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Activation("softmax"))

model.compile(optimizer="adam", loss="mean_squared_error", metrics=["mean_squared_error"])

for i in range(600):

  model.fit(x_train, y_train, epochs=1, batch_size=32, validation_data=(x_val, y_val))

#model.fit(x_train, y_train, batch_size=32, epochs=18)

  #print(x_val)

  y_pred=model.predict(x_val)
  print("y_val", y_val)
  print("y_pred", y_pred)
 
 

  error = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
  print('Relative L2 Norm: %e' % (error))



'''
result = model.fit(x_train, y_train, epochs=32, batch_size=18, validation_data=(x_val, y_val))

# enable early stopping based on mean_squared_error
#earlystopping=EarlyStopping(monitor="mean_squared_error", patience=40, verbose=1, mode='auto')
# fit model
#result = model.fit(x_train, y_train, epochs=240, batch_size=5, validation_data=(x_test, y_test), callbacks=[earlystopping])

y_pred=model.predict(x_val)

 

#test_loss, test_acc = model.evaluate(x_val, y_val)

#print(f"Test accuracy {test_acc * 100:.2f} %")

error_y = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
 
print('Error y: %e' % (error_y))'''


'''model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=["mean_squared_error"])


model.fit(x_train, y_train, batch_size=64, epochs=36)

y_pred=model.predict(x_val)

error_y = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
 
print('Error y: %e' % (error_y))

test_loss, test_acc = model.evaluate(x_val, y_val)

print(f"Test accuracy {test_acc * 100:.2f} %")'''

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/BBMgeHeightBouncing.mat')
    
x = data['BT1'].flatten()[:,None]
x=x.reshape(1000, 100000)
ht = data['PH1'].flatten()[:,None]
ht=ht.reshape(1000, 100000)
#t=t[:25]
#x = data['H'].flatten()[:,None]
#x=x.reshape(12, 200001)

#xx=np.zeros((16,2782))
xx=np.zeros((1000,25))
h=np.zeros((1000,1))
t=np.zeros((25, 1))
i=0
for x1 in x:
  xx[i]=x1[:25]
  h[i]=ht[i:i+1,0:1]
  i=i+1
  
for j in range(25):
  t[j]=j
 
print("h shape", h.shape)
print(h)
print("t shape", t.shape)
print(t)
H, T = np.meshgrid(h,t)

#H=(H-90)/0.275 - 1
 
#T=-1+T/2.1425
 
print("H", H.shape)
print(H)

print("T", T.shape)
print(T)

x_train_val= np.hstack((H.flatten()[:,None], T.flatten()[:,None]))
print("x_train_val", x_train_val.shape)
print(x_train_val)
#x_train_val= np.hstack((H, T))

#y_train_val=xx.T.flatten()[:,None]
y_train_val=xx.T.flatten()[:,None]
#y_train_val=-1+(y_train_val-20)/37.5

x_train=x_train_val[:-5000,:]
print("x_train", x_train.shape)
x_val=x_train_val[-5000:,:]


print("x_val", x_val.shape)
print(x_val)
 
 
y_train=y_train_val[:-5000,:]
y_val=y_train_val[-5000:,:]

print("y_val", y_val.shape)
print(y_val)

import larq as lq
# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer="ste_sign",
              kernel_quantizer="ste_sign",
              kernel_constraint="weight_clip")

model = tf.keras.models.Sequential()

# In the first layer we only quantize the weights and not the input
model.add(lq.layers.QuantDense(100,
                                kernel_quantizer="ste_sign",
                                kernel_constraint="weight_clip",
                                use_bias=False,
                                input_shape=(2,)))
#model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
#model.add(tf.keras.layers.MaxPooling2D(2))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Flatten())

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(1, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Activation("softmax"))


#model.compile(optimizer='adam',
             # loss='mean_squared_error', metrics=['mean_squared_error'])

model.compile(optimizer="Nadam", loss="mean_squared_error", metrics=["mean_squared_error"])

for i in range(6000):

  model.fit(x_train, y_train, epochs=1, batch_size=8, validation_data=(x_val, y_val))

#model.fit(x_train, y_train, batch_size=32, epochs=18)

  #print(x_val)

  y_pred=model.predict(x_val)

  print("y_pred", y_pred)
 
  print("y_val", y_val)

  error = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
  print('Error u: %e' % (error))
 
'''
result = model.fit(x_train, y_train, epochs=9, batch_size=12, validation_data=(x_val, y_val))

#model.fit(x_train, y_train, batch_size=32, epochs=18)

y_pred=model.predict(x_val)

print("y_pred", y_pred.shape)
print(y_pred)
print("y_val", y_val)'''

'''
with lq.context.quantized_scope(True):
    model.save("binary_model.h5")  # save binary weights
    weights = model.get_weights()
    print(weights)

test_loss, test_acc = model.evaluate(x_val, y_val)

print(f"Test accuracy {test_acc * 100:.2f} %")'''

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/BBAll.mat')
    
t = data['T'].flatten()[:,None]
t=t[:2782]
x = data['H'].flatten()[:,None]
x=x.reshape(16, 200001)
xx=np.zeros((16,2782))
h=np.zeros((16,1))
i=0
for x1 in x:
  initialHeight=20+i*0.5
  h[i]=initialHeight
  xx[i]=np.insert(x1[:2781], 0, initialHeight)
  i=i+1
 

H, T = np.meshgrid(h,t)

#H=-1+(H-20)/37.5
#T=-1+T/6.9525
#H=H.T.flatten()[:,None]
#T=T.T.flatten()[:,None]
print("H", H.shape)
print(H)

print("T", T.shape)
print(T)

x_train_val= np.hstack((H.flatten()[:,None], T.flatten()[:,None]))
print("x_train_val", x_train_val.shape)
print(x_train_val)
#x_train_val= np.hstack((H, T))

#y_train_val=xx.T.flatten()[:,None]
y_train_val=xx.T.flatten()[:,None]
#y_train_val=-1+(y_train_val-20)/37.5

x_train=x_train_val[:-5000,:]
x_val=x_train_val[-5000:,:]

print("x_val", x_val.shape)
print(x_val)
 
 
y_train=y_train_val[:-5000,:]
y_val=y_train_val[-5000:,:]

print("y_val", y_val.shape)
print(y_val)

import larq as lq
# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer="ste_sign",
              kernel_quantizer="ste_sign",
              kernel_constraint="weight_clip")

model = tf.keras.models.Sequential()

# In the first layer we only quantize the weights and not the input
model.add(lq.layers.QuantDense(100,
                                #kernel_quantizer="ste_sign",
                                #kernel_constraint="weight_clip",
                                use_bias=False,
                                input_shape=(2,)))
#model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False))
#model.add(tf.keras.layers.MaxPooling2D(2))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(100, use_bias=False))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Flatten())

model.add(lq.layers.QuantDense(100, use_bias=False))
model.add(tf.keras.layers.BatchNormalization(scale=False))
model.add(lq.layers.QuantDense(1, use_bias=False))
#model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Activation("softmax"))


model.compile(optimizer='adam',
              loss='mean_squared_logarithmic_error',
              metrics=['accuracy',])

model.fit(x_train, y_train, batch_size=32, epochs=18)

print("x_val shapes", x_val.shape)
print(x_val)

y_pred=model.predict(x_val)

#format_float = "{:.7f}".format(y_pred[9999][0])
#print(format_float)

print("y_pred", y_pred.shape)
print(y_pred)
print("y_val", y_val)

'''
with lq.context.quantized_scope(True):
    model.save("binary_model.h5")  # save binary weights
    weights = model.get_weights()
    print(weights)

test_loss, test_acc = model.evaluate(x_val, y_val)

print(f"Test accuracy {test_acc * 100:.2f} %")'''

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
print(train_images.shape)
print(train_labels.shape)


train_images = train_images.reshape((60000, 28, 28, 1))
test_images = test_images.reshape((10000, 28, 28, 1))
print(train_images)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
 


import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/BBMge.mat')

t = data['T'].flatten()[:,None]
#t=t[:403]
t=t[:500]
x = data['H1'].flatten()[:,None]
x=x.reshape(1000, 200001)
x=x[:500,:]

#xx=np.zeros((16,2782))
xx=np.zeros((500,500))
h=np.zeros((500,1))
i=0
for x1 in x:
  xx[i]=x1[:500]
  h[i]=x1[0]
  i=i+1

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
print(tf.__version__)
import numpy as np
import scipy.io
from scipy.interpolate import griddata
import time 

np.random.seed(1234)
tf.set_random_seed(1234)


data = scipy.io.loadmat('sample_data/BBMge.mat')
    


t = data['T'].flatten()[:,None]
#t=t[:403]
t=t[:500]
x = data['H1'].flatten()[:,None]
x=x.reshape(1000, 200001)
x=x[:500,:]

#xx=np.zeros((16,2782))
xx=np.zeros((500,500))
h=np.zeros((500,1))
i=0
for x1 in x:
  xx[i]=x1[:500]
  h[i]=x1[0]
  i=i+1
  
 
 
H, T = np.meshgrid(h,t)

H=(H-60)/2.495 - 1 #(H-min)/((max-min)/2)-1
 
T=-1+T/1.2475 #T/(max/2)-1
 
print("H", H.shape)
print(H)

print("T", T.shape)
print(T)

#x_train_val= np.hstack((H.flatten()[:,None], T.flatten()[:,None]))
x_train_val= np.hstack((H.T.flatten()[:,None], T.T.flatten()[:,None]))
print("x_train_val", x_train_val.shape)
print(x_train_val)
#x_train_val= np.hstack((H, T))

#y_train_val=xx.T.flatten()[:,None]
y_train_val=xx.flatten()[:,None]
#y_train_val=-1+(y_train_val-20)/37.5

x_train=x_train_val[:-30000,:]
x_val=x_train_val[-30000:,:]

print("x_val", x_val.shape)
print(x_val)
 
 
y_train=y_train_val[:-30000,:]
y_val=y_train_val[-30000:,:]

print("y_val", y_val.shape)
print(y_val)

import larq as lq
from keras.layers import Dropout

# All quantized layers except the first will use the same options
kwargs = dict(input_quantizer="ste_sign",
              kernel_quantizer="ste_sign",
              kernel_constraint="weight_clip")

model = tf.keras.models.Sequential()

# In the first layer we only quantize the weights and not the input
model.add(lq.layers.QuantDense(100,
                                kernel_quantizer="ste_sign",
                                kernel_constraint="weight_clip",
                                use_bias=False,
                                input_shape=(2,)))
#model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.BatchNormalization(scale=False))
 

model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
#model.add(tf.keras.layers.MaxPooling2D(2))
model.add(tf.keras.layers.BatchNormalization(scale=False))
 
model.add(lq.layers.QuantDense(100, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
 
#model.add(tf.keras.layers.Flatten())

model.add(lq.layers.QuantDense(50, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))

model.add(lq.layers.QuantDense(1, use_bias=False, **kwargs))
model.add(tf.keras.layers.BatchNormalization(scale=False))
#model.add(tf.keras.layers.Activation("softmax"))


#model.compile(optimizer='adam',
#              loss='mean_squared_error',
#              metrics=['accuracy',])

model.compile(optimizer="adam", loss="mean_squared_error", metrics=["mean_squared_error"])

for i in range(60):

  model.fit(x_train, y_train, epochs=1, batch_size=8, validation_data=(x_val, y_val))

#model.fit(x_train, y_train, batch_size=32, epochs=18)

  #print(x_val)

  y_pred=model.predict(x_val)

  print("y_pred", y_pred)
 
  print("y_val", y_val)

  error = np.linalg.norm(y_val-y_pred,2)/np.linalg.norm(y_val,2)
  print('Error u: %e' % (error))
 

'''

model.fit(x_train, y_train, epochs=1, batch_size=8, validation_data=(x_val, y_val))

y_pred=model.predict(x_val)

print("y_pred", y_pred)
 
print("y_val", y_val)

model.fit(x_train, y_train, epochs=1, batch_size=8, validation_data=(x_val, y_val))

y_pred=model.predict(x_val)

print("y_pred", y_pred)
 
print("y_val", y_val)

#format_float = "{:.7f}".format(y_pred[9999][0])
#print(format_float)'''